import asyncio
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode, tools_condition
from langchain_openai import ChatOpenAI
import os
from dotenv import load_dotenv

load_dotenv()

api_key = os.getenv("API_KEY")

async def main():
    print("ğŸš€ Starting MCP Client Example...")
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = ChatOpenAI(
        model="deepseek-v3-1-250821",
        api_key=api_key,
        base_url="https://ark.cn-beijing.volces.com/api/v3",
    )

    # è®¾ç½®MCPå®¢æˆ·ç«¯ï¼Œè¿æ¥å¤šä¸ªæœåŠ¡å™¨
    client = MultiServerMCPClient(
        {
            "math": {
                "command": "python",
                "args": ["/Users/rolex/Dev/workspace/python/python_learning/third-party/langgraph/mcp/math_tool_mcp_server.py"],
                "transport": "stdio",
            },
            "host_info": {
                "command": "python",
                "args": ["/Users/rolex/Dev/workspace/python/python_learning/third-party/langgraph/mcp/host_info_tool_mcp_server.py"],
                "transport": "stdio",
            },
            "weather": {
                "url": "http://localhost:8000/mcp",
                "transport": "streamable_http",
            }
        }
    )
    
    # è·å–æ‰€æœ‰å¯ç”¨çš„å·¥å…·
    tools = await client.get_tools()
    print(f"ğŸ“‹ Available tools: {[tool.name for tool in tools]}")
    
    # å°†å·¥å…·ç»‘å®šåˆ°æ¨¡å‹
    model_with_tools = model.bind_tools(tools)
    
    # åˆ›å»ºå·¥å…·èŠ‚ç‚¹
    tool_node = ToolNode(tools)
    
    # å®šä¹‰æ¨¡å‹è°ƒç”¨å‡½æ•°
    async def call_model(state: MessagesState):
        messages = state["messages"]
        response = await model_with_tools.ainvoke(messages)
        return {"messages": [response]}
    
    # æ„å»ºå›¾
    builder = StateGraph(MessagesState)
    builder.add_node("call_model", call_model)
    builder.add_node("tools", tool_node)
    
    builder.add_edge(START, "call_model")
    builder.add_conditional_edges(
        "call_model",
        tools_condition,
        {"tools": "tools", END: END}
    )
    builder.add_edge("tools", "call_model")
    
    # ç¼–è¯‘å›¾
    graph = builder.compile()
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        "è®¡ç®— (15 + 7) Ã— 3 çš„ç»“æœ",
        "åŒ—äº¬ç°åœ¨çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "è®¡ç®—åŠå¾„ä¸º5çš„åœ†çš„é¢ç§¯",
        "æ¯”è¾ƒåŒ—äº¬å’Œä¸Šæµ·çš„å¤©æ°”",
        "è®¡ç®—10çš„é˜¶ä¹˜",
        "æˆ‘çš„ç”µè„‘å†…å­˜æ˜¯å¤šå¤§"
    ]
    
    for i, query in enumerate(test_cases, 1):
        print(f"\nğŸ” Test {i}: {query}")
        print("-" * 50)
        
        try:
            result = await graph.ainvoke({"messages": [{"role": "user", "content": query}]})
            
            # æå–æœ€ç»ˆå›å¤
            final_message = result["messages"][-1]
            if hasattr(final_message, 'content'):
                print(f"ğŸ¤– AIå›å¤: {final_message.content}")
            else:
                print(f"ğŸ“Š å·¥å…·è°ƒç”¨ç»“æœ: {final_message}")
                
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            print("ğŸ’¡ æç¤º: è¯·ç¡®ä¿MCPæœåŠ¡å™¨æ­£åœ¨è¿è¡Œ:")
            print("  æ•°å­¦æœåŠ¡å™¨: python math_tool_mcp_server.py")
            print("  å¤©æ°”æœåŠ¡å™¨: python weather_tool_mcp_server.py")
            break
    
    print("\nâœ… ç¤ºä¾‹å®Œæˆ!")

if __name__ == "__main__":
    asyncio.run(main())